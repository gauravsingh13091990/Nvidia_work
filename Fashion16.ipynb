{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST 'FP16'\n",
    "### Goal \n",
    "##### 1) keras classification model on MNIST Fashion Dataset \n",
    "##### 3) Otimizing the pb graph of the model using tensorrt \n",
    "##### 4) Checking the performance difference b/w tensorflow and tensorrt while doing inferencing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Dataset Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the fashion-mnist pre-shuffled train data and test data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f283cc25be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show one of the images from the training dataset\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n",
    "A keras model using conv2d layer is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 11:44:19.213308 139817274914560 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first (Conv2D)               (None, 28, 28, 128)       640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 32)          8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               73984     \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 118,250\n",
      "Trainable params: 118,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1),name='first')) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu')) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax',name='last'))\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### model is compiled along with Adam optimizer along with categorical_crossentropy  for loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the train dataset into train and validation as crossentropy take binary input so all the classes are converted in binary form using to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50004, 28, 28)\n",
      "(50004,)\n",
      "(60000, 28, 28)\n",
      "(50004, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, y_val = train_test_split(x_train, y_train, test_size=0.1666, random_state=42)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(x_train.shape)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model training checking its accuracy validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50004 samples, validate on 9996 samples\n",
      "Epoch 1/10\n",
      "50004/50004 [==============================] - 16s 313us/sample - loss: 0.5547 - acc: 0.7971 - val_loss: 0.3771 - val_acc: 0.8675\n",
      "Epoch 2/10\n",
      "50004/50004 [==============================] - 15s 306us/sample - loss: 0.3387 - acc: 0.8769 - val_loss: 0.3342 - val_acc: 0.8767\n",
      "Epoch 3/10\n",
      "50004/50004 [==============================] - 15s 303us/sample - loss: 0.2928 - acc: 0.8926 - val_loss: 0.2811 - val_acc: 0.8976\n",
      "Epoch 4/10\n",
      "50004/50004 [==============================] - 16s 311us/sample - loss: 0.2635 - acc: 0.9035 - val_loss: 0.2722 - val_acc: 0.9001\n",
      "Epoch 5/10\n",
      "50004/50004 [==============================] - 15s 306us/sample - loss: 0.2400 - acc: 0.9108 - val_loss: 0.2477 - val_acc: 0.9107\n",
      "Epoch 6/10\n",
      "50004/50004 [==============================] - 15s 306us/sample - loss: 0.2254 - acc: 0.9168 - val_loss: 0.2524 - val_acc: 0.9075\n",
      "Epoch 7/10\n",
      "50004/50004 [==============================] - 16s 315us/sample - loss: 0.2082 - acc: 0.9234 - val_loss: 0.2509 - val_acc: 0.9066\n",
      "Epoch 8/10\n",
      "50004/50004 [==============================] - 16s 311us/sample - loss: 0.1961 - acc: 0.9267 - val_loss: 0.2318 - val_acc: 0.9158\n",
      "Epoch 9/10\n",
      "50004/50004 [==============================] - 16s 313us/sample - loss: 0.1832 - acc: 0.9315 - val_loss: 0.2411 - val_acc: 0.9120\n",
      "Epoch 10/10\n",
      "50004/50004 [==============================] - 16s 311us/sample - loss: 0.1721 - acc: 0.9354 - val_loss: 0.2440 - val_acc: 0.9109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f283f90dba8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train= X_train.reshape(-1,28,28,1)\n",
    "X_val = X_val.reshape(-1,28,28,1)\n",
    "model.fit(X_train,\n",
    "         Y_train,\n",
    "         batch_size=64,\n",
    "         epochs=10,\n",
    "         validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating accuracy on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.9051\n"
     ]
    }
   ],
   "source": [
    "y_test=to_categorical(y_test)\n",
    "x_test =x_test.reshape(-1,28,28,1)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To save trained model weights and biases h5 format file is saved in directory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/modelLeNet5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(0) # Required when there is batch normalization layer in the model\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"last/Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "name1 =[node.op.name for node in model.outputs]\n",
    "print(model.get_layer('last').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/gauravtrt/Fashion16'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As keras use tensorflow in backend so by using backed.get_seesion we create meta file data file checkpoint and index using h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0716 11:46:56.080512 139817274914560 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0716 11:46:56.082267 139817274914560 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras model sucessfully converted to tf/workspace/gauravtrt/Fashion16\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"/workspace/gauravtrt/Fashion16\"\n",
    "model = load_model('/workspace/gauravtrt/Fashion16/model/modelLeNet5.h5')\n",
    "\n",
    "# save the model to tensorflow\n",
    "saver=tf.train.Saver()\n",
    "sess =tf.keras.backend.get_session()\n",
    "save_path=saver.save(sess,MODEL_PATH+'/Project_data')\n",
    "print(\"keras model sucessfully converted to tf\"+MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using meta and data file pb frozen graph file is created by freezing all variables to constant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen model is successfully stored!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.tensorrt as trt\n",
    "from tensorflow.python.platform import gfile\n",
    "os.getcwd()\n",
    "\n",
    "tf.enable_resource_variables()\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "    saver = tf.train.import_meta_graph(\"/workspace/gauravtrt/Fashion16/Project_data.meta\")\n",
    "    saver.restore(sess, \"/workspace/gauravtrt/Fashion16/Project_data\")\n",
    "    your_outputs = ['last/Softmax']\n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(sess,tf.get_default_graph().as_graph_def(),\n",
    "        output_node_names=your_outputs)\n",
    "    with gfile.FastGFile(\"/workspace/gauravtrt/Fashion16/model/frozen_model.pb\", 'wb') as f:\n",
    "        f.write(frozen_graph.SerializeToString())\n",
    "    print(\"Frozen model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we use 1000 as batch size and precision mode \"FP16\" for creating tensorrt pb graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT model is successfully stored!\n"
     ]
    }
   ],
   "source": [
    "# convert (optimize) frozen model to TensorRT model\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,# frozen model\n",
    "    outputs=your_outputs,\n",
    "    max_batch_size=1000,# specify your max batch size\n",
    "    max_workspace_size_bytes=3*(10**9),# specify the max workspace\n",
    "    precision_mode=\"FP16\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\"\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "tf.enable_resource_variables()\n",
    "with gfile.FastGFile(\"./model/TensorRT_model.pb\", 'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "print(\"TensorRT model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can notice below number of nodes in tensorRT graph is much lesser then the frozen graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numb. of all_nodes in frozen graph: 47\n",
      "numb. of trt_engine_nodes in TensorRT graph: 0\n",
      "numb. of all_nodes in TensorRT graph: 37\n"
     ]
    }
   ],
   "source": [
    "# check how many ops of the original frozen model\n",
    "all_nodes = len([1 for n in frozen_graph.node])\n",
    "print(\"numb. of all_nodes in frozen graph:\", all_nodes)\n",
    "\n",
    "# check how many ops that is converted to TensorRT engine\n",
    "trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\n",
    "print(\"numb. of trt_engine_nodes in TensorRT graph:\", trt_engine_nodes)\n",
    "all_nodes = len([1 for n in trt_graph.node])\n",
    "print(\"numb. of all_nodes in TensorRT graph:\", all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt # must import this although we will not use it explicitly\n",
    "from tensorflow.python.platform import gfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "input_img=x_test\n",
    "print(input_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a \".pb\" model \n",
    "# (can be used to read frozen model or TensorRT model)\n",
    "def read_pb_graph(model):\n",
    "  with gfile.FastGFile(model,'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the inference time using tensorRT pb file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.87589430809021\n",
      "average inference time:  0.87589430809021\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "TENSORRT_MODEL_PATH = './model/TensorRT_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('first_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('last/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 1\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "            \n",
    "        avg_time_tensorRT = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference time for tensorRt model is 0.87589430809021 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the inference time using tensorflow frozen model pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.9845216274261475\n",
      "average inference time:  0.9845216274261475\n",
      "TensorRT improvement compared to the original model: 1.1240187524140752\n"
     ]
    }
   ],
   "source": [
    "FROZEN_MODEL_PATH = './model/frozen_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # read TensorRT model\n",
    "        frozen_graph = read_pb_graph(FROZEN_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(frozen_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('first_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('last/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 1\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \",delta_time) \n",
    "            #print(delta_time)\n",
    "        avg_time_original_model = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_original_model)\n",
    "        print(\"TensorRT improvement compared to the original model:\", avg_time_original_model/avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see from the above model that tensorRt inference time is 12% faster here on such a small model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size 1 and \"FP16\" as precision Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT model is successfully stored!\n"
     ]
    }
   ],
   "source": [
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,# frozen model\n",
    "    outputs=your_outputs,\n",
    "    max_batch_size=1,# specify your max batch size\n",
    "    max_workspace_size_bytes=3*(10**9),# specify the max workspace\n",
    "    precision_mode=\"FP16\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\"\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "tf.enable_resource_variables()\n",
    "with gfile.FastGFile(\"./model/TensorRT_model.pb\", 'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "print(\"TensorRT model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numb. of all_nodes in frozen graph: 47\n",
      "numb. of trt_engine_nodes in TensorRT graph: 0\n",
      "numb. of all_nodes in TensorRT graph: 37\n"
     ]
    }
   ],
   "source": [
    "# check how many ops of the original frozen model\n",
    "all_nodes = len([1 for n in frozen_graph.node])\n",
    "print(\"numb. of all_nodes in frozen graph:\", all_nodes)\n",
    "\n",
    "# check how many ops that is converted to TensorRT engine\n",
    "trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\n",
    "print(\"numb. of trt_engine_nodes in TensorRT graph:\", trt_engine_nodes)\n",
    "all_nodes = len([1 for n in trt_graph.node])\n",
    "print(\"numb. of all_nodes in TensorRT graph:\", all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.888181209564209\n",
      "average inference time:  0.888181209564209\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "TENSORRT_MODEL_PATH = './model/TensorRT_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('first_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('last/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 1\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "            \n",
    "        avg_time_tensorRT = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.9398398399353027\n",
      "average inference time:  0.9398398399353027\n",
      "TensorRT improvement compared to the original model: 1.0581622644284947\n"
     ]
    }
   ],
   "source": [
    "FROZEN_MODEL_PATH = './model/frozen_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # read TensorRT model\n",
    "        frozen_graph = read_pb_graph(FROZEN_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(frozen_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('first_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('last/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 1\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \",delta_time) \n",
    "            #print(delta_time)\n",
    "        avg_time_original_model = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_original_model)\n",
    "        print(\"TensorRT improvement compared to the original model:\", avg_time_original_model/avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using reduced batch size we are able to get 5% faster inference using tensorRt then tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
